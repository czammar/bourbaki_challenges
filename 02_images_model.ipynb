{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MI30743\\AppData\\Local\\Temp\\1\\ipykernel_27772\\753830450.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import (\n",
    "    Module,\n",
    "    Conv2d,\n",
    "    BatchNorm2d,\n",
    "    MaxPool2d,\n",
    "    Dropout2d,\n",
    "    Linear,\n",
    "    BatchNorm1d,\n",
    "    ReLU,\n",
    "    Dropout,\n",
    "    CrossEntropyLoss,\n",
    ")\n",
    "from torchsummary import summary\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics & Calibration\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    brier_score_loss,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGS_STRUCTURE_CSV = os.path.join(\n",
    "    os.getcwd(),'data\\\\custom_data\\\\',\n",
    "    \"images_reorganization.csv\"\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reorg = pd.read_csv(IMGS_STRUCTURE_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>group</th>\n",
       "      <th>label</th>\n",
       "      <th>folder_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE00409_170552_00_1_2_2001.jpg</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE00408_120515_00_3_4_2001.jpg</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE00227_212425_00_3_4_2001.jpg</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE00309_083542_00_2_1_2001.jpg</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE00305_112326_00_1_3_2001.jpg</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>AE00227_033425_00_3_1_2001.jpg</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10605</th>\n",
       "      <td>AE00531_061059_00_4_3_2001.jpg</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10606</th>\n",
       "      <td>AE00343_085315_00_2_2_2001.jpg</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10607</th>\n",
       "      <td>AE00305_144800_00_2_2_2001.jpg</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10608</th>\n",
       "      <td>AE00427_214514_00_3_2_2001.jpg</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10609 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               images  group  label  folder_label\n",
       "0      AE00409_170552_00_1_2_2001.jpg  train      1             2\n",
       "1      AE00408_120515_00_3_4_2001.jpg  train      0             1\n",
       "2      AE00227_212425_00_3_4_2001.jpg  train      1             2\n",
       "3      AE00309_083542_00_2_1_2001.jpg  train      1             2\n",
       "4      AE00305_112326_00_1_3_2001.jpg  train      1             2\n",
       "...                               ...    ...    ...           ...\n",
       "10604  AE00227_033425_00_3_1_2001.jpg   test      1             2\n",
       "10605  AE00531_061059_00_4_3_2001.jpg   test      1             2\n",
       "10606  AE00343_085315_00_2_2_2001.jpg   test      1             2\n",
       "10607  AE00305_144800_00_2_2_2001.jpg   test      1             2\n",
       "10608  AE00427_214514_00_3_2_2001.jpg   test      1             2\n",
       "\n",
       "[10609 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reorg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reorg.query(\"group == 'train'\").iloc[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SIZE = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.Resize(SIZE),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, group='train', transform=None) -> None:\n",
    "\n",
    "        self.group = group\n",
    "        self.df = pd.read_csv(csv_file).query(\n",
    "            f\"group == '{self.group}'\"\n",
    "            )\n",
    "        self.transform = transform\n",
    "        self.targets = self.df.label.to_list()\n",
    "        self.class_to_idx = Counter(self.df.label.to_list())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def load_image(self, index) -> Image.Image:\n",
    "        \n",
    "        image_name = os.path.join(\n",
    "            os.getcwd(),'data\\\\custom_data\\\\',\n",
    "            self.group,\n",
    "            self.df.iloc[index, 0]\n",
    "            )\n",
    "        \n",
    "        return Image.open(image_name)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.load_image(index)\n",
    "        label = int(self.df.iloc[index, 2])\n",
    "\n",
    "        if self.transform:\n",
    "            return self.transform(image), label\n",
    "        else:\n",
    "            return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(csv_file=IMGS_STRUCTURE_CSV, group='train', transform=transforms)\n",
    "val_dataset = CustomDataset(csv_file=IMGS_STRUCTURE_CSV, group='val', transform=transforms)\n",
    "test_dataset = CustomDataset(csv_file=IMGS_STRUCTURE_CSV, group='test', transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Neural Net\n",
    "do = 0.3\n",
    "input = 1\n",
    "output = 32\n",
    "kernel = 3\n",
    "target = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2D1 = Conv2d(input, output, kernel_size=kernel, padding=0, bias=False)\n",
    "        self.conv2D2 = Conv2d(\n",
    "            output, output * 2, kernel_size=kernel, padding=0, bias=False\n",
    "        )\n",
    "        self.conv2D3 = Conv2d(\n",
    "            output * 2, output * 4, kernel_size=kernel, padding=0, bias=False\n",
    "        )\n",
    "        self.conv2D4 = Conv2d(\n",
    "            output * 4, output * 8, kernel_size=kernel, padding=0, bias=False\n",
    "        )\n",
    "        self.conv2D5 = Conv2d(\n",
    "            output * 8, output * 16, kernel_size=kernel, padding=0, bias=False\n",
    "        )\n",
    "        self.bn2D1 = BatchNorm2d(output)\n",
    "        self.bn2D2 = BatchNorm2d(output * 2)\n",
    "        self.bn2D3 = BatchNorm2d(output * 4)\n",
    "        self.bn2D4 = BatchNorm2d(output * 8)\n",
    "        self.bn2D5 = BatchNorm2d(output * 16)\n",
    "        self.relu = ReLU()\n",
    "        self.maxpool = MaxPool2d(kernel_size=kernel - 1, stride=2)\n",
    "        self.dropout2D = Dropout2d(do/2)\n",
    "        self.dense1 = Linear(2048, output * 8)\n",
    "        self.dense2 = Linear(output * 8, output * 4)\n",
    "        self.bn1D1 = BatchNorm1d(output * 8)\n",
    "        self.bn1D2 = BatchNorm1d(output * 4)\n",
    "        self.dropout1D = Dropout(do)\n",
    "        self.dense3 = Linear(output * 4, target)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout2D(self.maxpool(self.relu(self.bn2D1(self.conv2D1(x)))))\n",
    "        x = self.dropout2D(self.maxpool(self.relu(self.bn2D2(self.conv2D2(x)))))\n",
    "        x = self.dropout2D(self.maxpool(self.relu(self.bn2D3(self.conv2D3(x)))))\n",
    "        x = self.maxpool(self.relu(self.bn2D4(self.conv2D4(x))))\n",
    "        x = self.maxpool(self.relu(self.bn2D5(self.conv2D5(x))))\n",
    "        x = x.view(\n",
    "            x.shape[0], -1\n",
    "        )  # Flatten dimensions except batch size to enter to the linear layer\n",
    "        x = self.dropout1D(self.relu(self.bn1D1(self.dense1(x))))\n",
    "        x = self.relu(self.bn1D2(self.dense2(x)))\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (conv2D1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (conv2D2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (conv2D3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (conv2D4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (conv2D5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (bn2D1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2D2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2D3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2D4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2D5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout2D): Dropout2d(p=0.15, inplace=False)\n",
       "  (dense1): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (dense2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (bn1D1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn1D2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1D): Dropout(p=0.3, inplace=False)\n",
       "  (dense3): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ImageClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [128, 32, 126, 126]             288\n",
      "       BatchNorm2d-2        [128, 32, 126, 126]              64\n",
      "              ReLU-3        [128, 32, 126, 126]               0\n",
      "         MaxPool2d-4          [128, 32, 63, 63]               0\n",
      "         Dropout2d-5          [128, 32, 63, 63]               0\n",
      "            Conv2d-6          [128, 64, 61, 61]          18,432\n",
      "       BatchNorm2d-7          [128, 64, 61, 61]             128\n",
      "              ReLU-8          [128, 64, 61, 61]               0\n",
      "         MaxPool2d-9          [128, 64, 30, 30]               0\n",
      "        Dropout2d-10          [128, 64, 30, 30]               0\n",
      "           Conv2d-11         [128, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-12         [128, 128, 28, 28]             256\n",
      "             ReLU-13         [128, 128, 28, 28]               0\n",
      "        MaxPool2d-14         [128, 128, 14, 14]               0\n",
      "        Dropout2d-15         [128, 128, 14, 14]               0\n",
      "           Conv2d-16         [128, 256, 12, 12]         294,912\n",
      "      BatchNorm2d-17         [128, 256, 12, 12]             512\n",
      "             ReLU-18         [128, 256, 12, 12]               0\n",
      "        MaxPool2d-19           [128, 256, 6, 6]               0\n",
      "           Conv2d-20           [128, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-21           [128, 512, 4, 4]           1,024\n",
      "             ReLU-22           [128, 512, 4, 4]               0\n",
      "        MaxPool2d-23           [128, 512, 2, 2]               0\n",
      "           Linear-24                 [128, 256]         524,544\n",
      "      BatchNorm1d-25                 [128, 256]             512\n",
      "             ReLU-26                 [128, 256]               0\n",
      "          Dropout-27                 [128, 256]               0\n",
      "           Linear-28                 [128, 128]          32,896\n",
      "      BatchNorm1d-29                 [128, 128]             256\n",
      "             ReLU-30                 [128, 128]               0\n",
      "           Linear-31                   [128, 2]             258\n",
      "================================================================\n",
      "Total params: 2,127,458\n",
      "Trainable params: 2,127,458\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 8.00\n",
      "Forward/backward pass size (MB): 3034.00\n",
      "Params size (MB): 8.12\n",
      "Estimated Total Size (MB): 3050.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (1, SIZE[0], SIZE[1]), batch_size=BATCH_SIZE, device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, verbose=False, delta=0, path=\"checkpoint.pt\"):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"Validation Loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model to {self.path}\"\n",
    "            )\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "    def load_checkpoint(self, model):\n",
    "        \"\"\"Loads the best model weights from the saved checkpoint.\"\"\"\n",
    "        model.load_state_dict(torch.load(self.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunners\n",
    "LR = 0.001\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: Counter({1: 4212, 0: 2577})\n",
      "Train Dataset: Counter({1: 4212, 0: 2577})\n",
      "Val Dataset: Counter({1: 1053, 0: 645})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classes: {train_dataset.class_to_idx}\")\n",
    "num_classes_train = Counter(train_dataset.targets)\n",
    "num_classes_val = Counter(val_dataset.targets)\n",
    "print(f\"Train Dataset: {num_classes_train}\")\n",
    "print(f\"Val Dataset: {num_classes_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class frequencies\n",
    "class_frequencies = torch.tensor(\n",
    "    [num_classes_train[0], num_classes_train[1]], dtype=torch.float32\n",
    ")  # Example frequencies for two classes\n",
    "\n",
    "# Calculate class weights\n",
    "total_samples = class_frequencies.sum()\n",
    "class_weights = total_samples / (class_frequencies * len(class_frequencies))\n",
    "class_weights[0]=class_weights[0]*5\n",
    "class_weights[1]=class_weights[1]/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR, amsgrad=True, weight_decay=LR * 0.1)\n",
    "criterion = CrossEntropyLoss(weight=class_weights).to(device)\n",
    "#criterion = BCEWithLogitsLoss(weight=class_weights).to(device)\n",
    "early_stopping = EarlyStopping(patience=int(EPOCHS * 0.5), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model, loader, optimizer, loss_func, is_training):\n",
    "    correct = 0\n",
    "    loss_list = []\n",
    "    stop = False\n",
    "    model.train() if is_training else model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(is_training):  # Enable gradients only in training\n",
    "        for batch_idx, (image, target) in tqdm(\n",
    "            enumerate(loader),\n",
    "            desc=\"Epoch_Train\" if is_training else \"Epoch_Val\",\n",
    "            total=len(loader),\n",
    "        ):\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()  # Reset gradients\n",
    "            output = model(image)\n",
    "            loss = loss_func(output, target)\n",
    "            if is_training:\n",
    "                loss.backward()  # Compute gradients\n",
    "                optimizer.step()  # Update weights\n",
    "            loss_list.append(loss.item())\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=False\n",
    "            )  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target).sum().item()\n",
    "\n",
    "    if not is_training:\n",
    "        stop = early_stopping(\n",
    "            (sum(loss_list) / len(loss_list)), model\n",
    "        )  # Early stopping decision\n",
    "\n",
    "    return loss_list, correct, stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc_loss(total_loss, loss, total_accuracy, correct, loader):\n",
    "    total_loss.append(sum(loss) / len(loss))\n",
    "    total_accuracy.append(100 * correct / len(loader.dataset))\n",
    "    return total_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists\n",
    "total_loss_train = []\n",
    "total_loss_val = []\n",
    "total_accuracy_train = []\n",
    "total_accuracy_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10429785fc394aa29c6db952382bc2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch_Train:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([128])) must be the same as input size (torch.Size([128, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m\n",
      "Cell \u001b[1;32mIn[21], line 18\u001b[0m, in \u001b[0;36mtrain_eval\u001b[1;34m(model, loader, optimizer, loss_func, is_training)\u001b[0m\n\u001b[0;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Reset gradients\u001b[39;00m\n\u001b[0;32m     17\u001b[0m output \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[0;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MI30743\\Documents\\bourbaki\\reto_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MI30743\\Documents\\bourbaki\\reto_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MI30743\\Documents\\bourbaki\\reto_1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MI30743\\Documents\\bourbaki\\reto_1\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3197\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m-> 3197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([128])) must be the same as input size (torch.Size([128, 2]))"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training and Evaluation loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train Iteration\n",
    "    loss_train, correct_train, _ = train_eval(\n",
    "        model, train_loader, optimizer, criterion, is_training=True\n",
    "    )\n",
    "    # Val Iteration\n",
    "    loss_val, correct_val, stop = train_eval(\n",
    "        model, val_loader, optimizer, criterion, is_training=False\n",
    "    )\n",
    "    # Calculate and record loss & accuracy for training and validation\n",
    "    total_loss_train, total_accuracy_train = calc_acc_loss(\n",
    "        total_loss_train,\n",
    "        loss_train,\n",
    "        total_accuracy_train,\n",
    "        correct_train,\n",
    "        train_loader,\n",
    "    )\n",
    "    # Calculate Loss and Accuracy\n",
    "    total_loss_val, total_accuracy_val = calc_acc_loss(\n",
    "        total_loss_val, loss_val, total_accuracy_val, correct_val, val_loader\n",
    "    )\n",
    "    # Print metrics per epoch\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {total_loss_train[-1]:.4f}, Val Loss: {total_loss_val[-1]:.4f}, Train Accuracy: {total_accuracy_train[-1]:.2f}%, Val Accuracy: {total_accuracy_val[-1]:.2f}%\"\n",
    "    )\n",
    "    if stop:\n",
    "        print(\"Early Stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(7, 7))\n",
    "ax1.plot(total_loss_train, \"b\", label=\"Total Train Loss\")\n",
    "ax1.plot(total_loss_val, \"g\", label=\"Total Val Loss\")\n",
    "ax1.set_ylim(0, 5)\n",
    "ax1.legend(loc=8)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(total_accuracy_train, \"b\", marker=\"8\", label=\"Total Train Accuracy\")\n",
    "ax2.plot(total_accuracy_val, \"g\", marker=\"s\", label=\"Total Val Accuracy\")\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.legend(loc=9)\n",
    "plt.title(\"CNN Training Convergence\", color=\"black\")\n",
    "ax1.set_xlabel(\"Training Iterations\")\n",
    "ax1.set_ylabel(f\"{criterion._get_name}\", color=\"black\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "exec_date = today.strftime(\"%d%m%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"./data/models/model_v1a_{exec_date}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = ImageClassifier()\n",
    "model_pred.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
    "model_pred.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    total_loss_test = []\n",
    "    test_probs = []\n",
    "    correct_test = 0\n",
    "    pred = torch.Tensor().to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, target) in tqdm(\n",
    "            enumerate(loader), desc=\"Test Epoch\", total=len(loader)\n",
    "        ):\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(image)\n",
    "            probs = torch.sigmoid(output)\n",
    "            test_probs.extend(probs.cpu().numpy())\n",
    "            loss = criterion(output, target)\n",
    "            total_loss_test.append(loss.item())\n",
    "            test_pred = output.argmax(dim=1, keepdim=False)\n",
    "            correct_test += test_pred.eq(target).sum().item()\n",
    "            pred = torch.cat((pred, test_pred), 0)\n",
    "        return total_loss_test, correct_test, pred, test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_test, correct_test, y_pred, probs_pred = test(model_pred, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss Test: {(sum(total_loss_test) / len(total_loss_test)):.3f}\")\n",
    "print(f\"Test Accuracy: {(100 * correct_test/ len(test_loader.dataset)):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [int(pred) for pred in y_pred.cpu().numpy().tolist()]\n",
    "prob_list = [list(array) for array in probs_pred]\n",
    "prob_pos = [prob_list[i][1] for i in range(len(prob_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"g\")\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_dataset.targets, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(test_dataset.targets, y_pred)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_dataset.targets, prob_pos)\n",
    "\n",
    "# Compute Area Under the Curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"Accuracy\",\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"F1\",\n",
    "    \"ROC AUC\",\n",
    "    \"ECE\",\n",
    "    \"Log Loss\",\n",
    "    \"Brier Loss\",\n",
    "]\n",
    "\n",
    "perf_df = pd.DataFrame(columns=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(\n",
    "    model_name, targets, predictions, prob_pos, perf_df, verbose\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates model performance and updates the performance dataframe with metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    precision = precision_score(targets, predictions, zero_division=0)\n",
    "    recall = recall_score(targets, predictions)\n",
    "    f1 = f1_score(targets, predictions)\n",
    "    roc_auc = roc_auc_score(targets, prob_pos)\n",
    "    logloss = log_loss(targets, prob_pos)\n",
    "    brier_loss = brier_score_loss(targets, prob_pos)\n",
    "\n",
    "    # Compute Expected Calibration Error (ECE)\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        targets, prob_pos, n_bins=20, strategy=\"uniform\"\n",
    "    )\n",
    "    ece = np.sum(np.abs(fraction_of_positives - mean_predicted_value)) / len(\n",
    "        mean_predicted_value\n",
    "    )\n",
    "\n",
    "    # Populate the performance dataframe\n",
    "    perf_df.loc[model_name, :] = [\n",
    "        accuracy,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1,\n",
    "        roc_auc,\n",
    "        ece,\n",
    "        logloss,\n",
    "        brier_loss,\n",
    "    ]\n",
    "\n",
    "    # Plot Calibration Curve and Histogram if verbose is True\n",
    "    if verbose:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        gs = GridSpec(2, 1)\n",
    "        ax_calibration_curve = fig.add_subplot(gs[0, :])\n",
    "        ax_histogram = fig.add_subplot(gs[1, :])\n",
    "\n",
    "        # Plot Calibration Curve\n",
    "        ax_calibration_curve.plot(\n",
    "            mean_predicted_value, fraction_of_positives, '-s', label=model_name\n",
    "        )\n",
    "        ax_calibration_curve.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "        ax_calibration_curve.set_title(f\"Calibration plot ({model_name})\")\n",
    "        ax_calibration_curve.set_xlabel(\"Mean predicted probability\")\n",
    "        ax_calibration_curve.set_ylabel(\"Fraction of positives\")\n",
    "        ax_calibration_curve.legend()\n",
    "\n",
    "        # Plot Histogram\n",
    "        ax_histogram.hist(\n",
    "            prob_pos, range=(0, 1), bins=20, label=model_name)\n",
    "        ax_histogram.set_title(f\"Probability Distribution ({model_name})\")\n",
    "        ax_histogram.set_xlabel(\"Mean predicted probability\")\n",
    "        ax_histogram.set_ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(\n",
    "    \"CNN\", test_dataset.targets, y_pred, prob_pos, perf_df, verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
