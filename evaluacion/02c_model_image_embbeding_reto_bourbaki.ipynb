{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vExaOtgt44tm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zDsMUxIv0-3d"
      },
      "outputs": [],
      "source": [
        "#PATH_TABULAR_DATA = \"./drive/MyDrive/real_estate_price_prediction_processed_09042024.csv\"\n",
        "PATH_TABULAR_DATA = \"./real_estate_price_prediction_processed_09042024.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7XqalGDN03B6"
      },
      "outputs": [],
      "source": [
        "data_tab = pd.read_csv(PATH_TABULAR_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rGtdnVcPFhFQ"
      },
      "outputs": [],
      "source": [
        "data_tab[[\"price_scaled\"]] = MinMaxScaler().fit_transform(data_tab[[\"price\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "HdneEMsi1jNQ",
        "outputId": "05ead0a7-0f1b-4383-9df0-41547c971c86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_annonce</th>\n",
              "      <th>property_type</th>\n",
              "      <th>approximate_latitude</th>\n",
              "      <th>approximate_longitude</th>\n",
              "      <th>city</th>\n",
              "      <th>postal_code</th>\n",
              "      <th>size</th>\n",
              "      <th>floor</th>\n",
              "      <th>land_size</th>\n",
              "      <th>energy_performance_value</th>\n",
              "      <th>...</th>\n",
              "      <th>nb_photos</th>\n",
              "      <th>has_a_balcony</th>\n",
              "      <th>nb_terraces</th>\n",
              "      <th>has_a_cellar</th>\n",
              "      <th>has_a_garage</th>\n",
              "      <th>has_air_conditioning</th>\n",
              "      <th>last_floor</th>\n",
              "      <th>upper_floors</th>\n",
              "      <th>price</th>\n",
              "      <th>price_scaled</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>35996577</td>\n",
              "      <td>0</td>\n",
              "      <td>43.643880</td>\n",
              "      <td>7.117183</td>\n",
              "      <td>8452</td>\n",
              "      <td>199</td>\n",
              "      <td>63.0</td>\n",
              "      <td>3.479524</td>\n",
              "      <td>3995.665362</td>\n",
              "      <td>205.385148</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>355000.0</td>\n",
              "      <td>0.145320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>35811033</td>\n",
              "      <td>0</td>\n",
              "      <td>45.695757</td>\n",
              "      <td>4.895610</td>\n",
              "      <td>8234</td>\n",
              "      <td>3347</td>\n",
              "      <td>90.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3995.665362</td>\n",
              "      <td>223.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>190000.0</td>\n",
              "      <td>0.072778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35731841</td>\n",
              "      <td>12</td>\n",
              "      <td>47.966791</td>\n",
              "      <td>-1.220451</td>\n",
              "      <td>5235</td>\n",
              "      <td>1650</td>\n",
              "      <td>61.0</td>\n",
              "      <td>3.479524</td>\n",
              "      <td>370.000000</td>\n",
              "      <td>205.385148</td>\n",
              "      <td>...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39000.0</td>\n",
              "      <td>0.006390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>35886765</td>\n",
              "      <td>12</td>\n",
              "      <td>47.289292</td>\n",
              "      <td>-1.878805</td>\n",
              "      <td>1929</td>\n",
              "      <td>2050</td>\n",
              "      <td>142.0</td>\n",
              "      <td>3.479524</td>\n",
              "      <td>764.000000</td>\n",
              "      <td>217.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>299000.0</td>\n",
              "      <td>0.120699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35781137</td>\n",
              "      <td>0</td>\n",
              "      <td>45.718992</td>\n",
              "      <td>4.844234</td>\n",
              "      <td>4476</td>\n",
              "      <td>3334</td>\n",
              "      <td>88.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3995.665362</td>\n",
              "      <td>205.385148</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>478000.0</td>\n",
              "      <td>0.199397</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id_annonce  property_type  approximate_latitude  approximate_longitude  \\\n",
              "0    35996577              0             43.643880               7.117183   \n",
              "1    35811033              0             45.695757               4.895610   \n",
              "2    35731841             12             47.966791              -1.220451   \n",
              "3    35886765             12             47.289292              -1.878805   \n",
              "4    35781137              0             45.718992               4.844234   \n",
              "\n",
              "   city  postal_code   size     floor    land_size  energy_performance_value  \\\n",
              "0  8452          199   63.0  3.479524  3995.665362                205.385148   \n",
              "1  8234         3347   90.0  3.000000  3995.665362                223.000000   \n",
              "2  5235         1650   61.0  3.479524   370.000000                205.385148   \n",
              "3  1929         2050  142.0  3.479524   764.000000                217.000000   \n",
              "4  4476         3334   88.0  3.000000  3995.665362                205.385148   \n",
              "\n",
              "   ...  nb_photos  has_a_balcony  nb_terraces  has_a_cellar  has_a_garage  \\\n",
              "0  ...        4.0            0.0          1.0           0.0           0.0   \n",
              "1  ...        8.0            0.0          0.0           0.0           0.0   \n",
              "2  ...        4.0            0.0          0.0           0.0           0.0   \n",
              "3  ...        8.0            0.0          1.0           0.0           0.0   \n",
              "4  ...        5.0            1.0          0.0           0.0           0.0   \n",
              "\n",
              "   has_air_conditioning  last_floor  upper_floors     price  price_scaled  \n",
              "0                   0.0         0.0           0.0  355000.0      0.145320  \n",
              "1                   0.0         0.0           0.0  190000.0      0.072778  \n",
              "2                   0.0         0.0           0.0   39000.0      0.006390  \n",
              "3                   0.0         0.0           0.0  299000.0      0.120699  \n",
              "4                   0.0         0.0           0.0  478000.0      0.199397  \n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_tab.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YbWxI-EgFT4E"
      },
      "outputs": [],
      "source": [
        "X_train_, X_test, y_train_, y_test= train_test_split(\n",
        "    data_tab.drop(columns=[\"price\", \"price_scaled\"]),\n",
        "    data_tab[\"price_scaled\"],\n",
        "    test_size=0.20,\n",
        "    random_state=123,\n",
        "    shuffle=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hAEOQUi_FT9V"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_,\n",
        "    y_train_,\n",
        "    test_size=0.20,\n",
        "    random_state=123,\n",
        "    shuffle=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qEZsei-nGb6U"
      },
      "outputs": [],
      "source": [
        "X_reorganized = pd.concat(\n",
        "    [\n",
        "        pd.DataFrame(X_train).assign(group=\"train\"),\n",
        "        pd.DataFrame(X_val).assign(group=\"val\"),\n",
        "        pd.DataFrame(X_test).assign(group=\"test\")\n",
        "    ]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "\n",
        "y_reorganized = pd.concat(\n",
        "    [\n",
        "        pd.DataFrame(y_train).assign(group=\"train\"),\n",
        "        pd.DataFrame(y_val).assign(group=\"val\"),\n",
        "        pd.DataFrame(y_test).assign(group=\"test\")\n",
        "    ]\n",
        ").reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "qBU61NfWHFxh",
        "outputId": "a601a139-8c26-4504-cae1-45a1e72e760b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_annonce</th>\n",
              "      <th>property_type</th>\n",
              "      <th>approximate_latitude</th>\n",
              "      <th>approximate_longitude</th>\n",
              "      <th>city</th>\n",
              "      <th>postal_code</th>\n",
              "      <th>size</th>\n",
              "      <th>floor</th>\n",
              "      <th>land_size</th>\n",
              "      <th>energy_performance_value</th>\n",
              "      <th>...</th>\n",
              "      <th>nb_boxes</th>\n",
              "      <th>nb_photos</th>\n",
              "      <th>has_a_balcony</th>\n",
              "      <th>nb_terraces</th>\n",
              "      <th>has_a_cellar</th>\n",
              "      <th>has_a_garage</th>\n",
              "      <th>has_air_conditioning</th>\n",
              "      <th>last_floor</th>\n",
              "      <th>upper_floors</th>\n",
              "      <th>group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28497</th>\n",
              "      <td>36050439</td>\n",
              "      <td>11</td>\n",
              "      <td>45.198299</td>\n",
              "      <td>5.734706</td>\n",
              "      <td>3641</td>\n",
              "      <td>1838</td>\n",
              "      <td>123.0</td>\n",
              "      <td>3.479524</td>\n",
              "      <td>3995.665362</td>\n",
              "      <td>161.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2697</th>\n",
              "      <td>36045457</td>\n",
              "      <td>0</td>\n",
              "      <td>44.869099</td>\n",
              "      <td>-0.622993</td>\n",
              "      <td>3906</td>\n",
              "      <td>1476</td>\n",
              "      <td>41.0</td>\n",
              "      <td>3.479524</td>\n",
              "      <td>3995.665362</td>\n",
              "      <td>205.385148</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5487</th>\n",
              "      <td>36050285</td>\n",
              "      <td>0</td>\n",
              "      <td>43.611005</td>\n",
              "      <td>3.879018</td>\n",
              "      <td>5127</td>\n",
              "      <td>1569</td>\n",
              "      <td>12181.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3995.665362</td>\n",
              "      <td>205.385148</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3144</th>\n",
              "      <td>36059213</td>\n",
              "      <td>12</td>\n",
              "      <td>44.498465</td>\n",
              "      <td>0.163925</td>\n",
              "      <td>4634</td>\n",
              "      <td>2192</td>\n",
              "      <td>127.0</td>\n",
              "      <td>3.479524</td>\n",
              "      <td>3995.665362</td>\n",
              "      <td>205.385148</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13987</th>\n",
              "      <td>35855667</td>\n",
              "      <td>12</td>\n",
              "      <td>50.673535</td>\n",
              "      <td>3.153938</td>\n",
              "      <td>2104</td>\n",
              "      <td>2717</td>\n",
              "      <td>181.0</td>\n",
              "      <td>3.479524</td>\n",
              "      <td>3995.665362</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id_annonce  property_type  approximate_latitude  approximate_longitude  \\\n",
              "28497    36050439             11             45.198299               5.734706   \n",
              "2697     36045457              0             44.869099              -0.622993   \n",
              "5487     36050285              0             43.611005               3.879018   \n",
              "3144     36059213             12             44.498465               0.163925   \n",
              "13987    35855667             12             50.673535               3.153938   \n",
              "\n",
              "       city  postal_code     size     floor    land_size  \\\n",
              "28497  3641         1838    123.0  3.479524  3995.665362   \n",
              "2697   3906         1476     41.0  3.479524  3995.665362   \n",
              "5487   5127         1569  12181.0  6.000000  3995.665362   \n",
              "3144   4634         2192    127.0  3.479524  3995.665362   \n",
              "13987  2104         2717    181.0  3.479524  3995.665362   \n",
              "\n",
              "       energy_performance_value  ...  nb_boxes  nb_photos  has_a_balcony  \\\n",
              "28497                161.000000  ...       0.0       12.0            0.0   \n",
              "2697                 205.385148  ...       0.0        2.0            1.0   \n",
              "5487                 205.385148  ...       0.0        4.0            0.0   \n",
              "3144                 205.385148  ...       0.0        6.0            0.0   \n",
              "13987                122.000000  ...       0.0       22.0            0.0   \n",
              "\n",
              "       nb_terraces  has_a_cellar  has_a_garage  has_air_conditioning  \\\n",
              "28497          1.0           0.0           0.0                   1.0   \n",
              "2697           0.0           0.0           0.0                   0.0   \n",
              "5487           1.0           0.0           0.0                   0.0   \n",
              "3144           1.0           0.0           0.0                   0.0   \n",
              "13987          0.0           0.0           0.0                   0.0   \n",
              "\n",
              "       last_floor  upper_floors  group  \n",
              "28497         0.0           0.0    val  \n",
              "2697          0.0           0.0  train  \n",
              "5487          0.0           0.0  train  \n",
              "3144          0.0           0.0  train  \n",
              "13987         0.0           0.0  train  \n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_reorganized.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b0szp2EOHF5t"
      },
      "outputs": [],
      "source": [
        "#X_reorganized[[\"id_annonce\", \"group\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-pJ7aY1HF87"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LBYLcEYD1pCN"
      },
      "outputs": [],
      "source": [
        "#PATH_IMAGES_DATA = \"./drive/MyDrive/real_estate_price_prediction_metadata_images_09042024.csv\"\n",
        "PATH_IMAGES_DATA = \"./real_estate_price_prediction_metadata_images_09042024.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NI8n22tl1pIT"
      },
      "outputs": [],
      "source": [
        "data_img = pd.read_csv(PATH_IMAGES_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xIGNQYpcHx2a"
      },
      "outputs": [],
      "source": [
        "data_img_agg = data_img.groupby('id_annonce')['long_path_image'].apply(list).reset_index(name='images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QKIvkALpHx5y"
      },
      "outputs": [],
      "source": [
        "data_img_groups = data_img_agg.merge(\n",
        "    X_reorganized[[\"id_annonce\", \"group\"]],\n",
        "    how='left',\n",
        "    on='id_annonce'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "O5DUrPL-Hx9-",
        "outputId": "6eeaf013-f315-4c4c-dd34-3b59b3e10768"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_annonce</th>\n",
              "      <th>images</th>\n",
              "      <th>group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30568</th>\n",
              "      <td>36028723</td>\n",
              "      <td>[C:\\Users\\MI30743\\github\\bourbaki_challenges-m...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34273</th>\n",
              "      <td>36050029</td>\n",
              "      <td>[C:\\Users\\MI30743\\github\\bourbaki_challenges-m...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7485</th>\n",
              "      <td>35793847</td>\n",
              "      <td>[C:\\Users\\MI30743\\github\\bourbaki_challenges-m...</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26294</th>\n",
              "      <td>35996833</td>\n",
              "      <td>[C:\\Users\\MI30743\\github\\bourbaki_challenges-m...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16905</th>\n",
              "      <td>35859735</td>\n",
              "      <td>[C:\\Users\\MI30743\\github\\bourbaki_challenges-m...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16300</th>\n",
              "      <td>35856069</td>\n",
              "      <td>[C:\\Users\\MI30743\\github\\bourbaki_challenges-m...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22522</th>\n",
              "      <td>35901959</td>\n",
              "      <td>[C:\\Users\\MI30743\\github\\bourbaki_challenges-m...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5303</th>\n",
              "      <td>35776053</td>\n",
              "      <td>[C:\\Users\\MI30743\\github\\bourbaki_challenges-m...</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8645</th>\n",
              "      <td>35801511</td>\n",
              "      <td>[C:\\Users\\MI30743\\github\\bourbaki_challenges-m...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24020</th>\n",
              "      <td>35949587</td>\n",
              "      <td>[C:\\Users\\MI30743\\github\\bourbaki_challenges-m...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id_annonce                                             images  group\n",
              "30568    36028723  [C:\\Users\\MI30743\\github\\bourbaki_challenges-m...  train\n",
              "34273    36050029  [C:\\Users\\MI30743\\github\\bourbaki_challenges-m...  train\n",
              "7485     35793847  [C:\\Users\\MI30743\\github\\bourbaki_challenges-m...    val\n",
              "26294    35996833  [C:\\Users\\MI30743\\github\\bourbaki_challenges-m...   test\n",
              "16905    35859735  [C:\\Users\\MI30743\\github\\bourbaki_challenges-m...  train\n",
              "16300    35856069  [C:\\Users\\MI30743\\github\\bourbaki_challenges-m...   test\n",
              "22522    35901959  [C:\\Users\\MI30743\\github\\bourbaki_challenges-m...  train\n",
              "5303     35776053  [C:\\Users\\MI30743\\github\\bourbaki_challenges-m...   test\n",
              "8645     35801511  [C:\\Users\\MI30743\\github\\bourbaki_challenges-m...  train\n",
              "24020    35949587  [C:\\Users\\MI30743\\github\\bourbaki_challenges-m...  train"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_img_groups.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t9qGRFnKWPbp"
      },
      "outputs": [],
      "source": [
        "# Define the image transformation\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "        ),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "-K0RgnK-M1NS"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[61], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m   image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m   image \u001b[38;5;241m=\u001b[39m preprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m   img_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_feature_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m   images_emmbeded\u001b[38;5;241m.\u001b[39mappend(img_embedding\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     26\u001b[0m images_embbeding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(images_emmbeded)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "Cell \u001b[1;32mIn[61], line 9\u001b[0m, in \u001b[0;36mget_model_embedding\u001b[1;34m(model_feature_extract, img_tensor)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_embedding\u001b[39m(model_feature_extract, img_tensor):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      8\u001b[0m           \u001b[38;5;66;03m# Get the image features from the ResNet-152 model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m           img_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_feature_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img_features\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torchvision\\models\\resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    144\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m--> 146\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m    148\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "mapeo = {}\n",
        "model_ = models.resnet152(pretrained=True)\n",
        "model_layers = list(model_.children())[:-1]\n",
        "model_feature_extract = torch.nn.Sequential(*model_layers)\n",
        "\n",
        "def get_model_embedding(model_feature_extract, img_tensor):\n",
        "    with torch.no_grad():\n",
        "          # Get the image features from the ResNet-152 model\n",
        "          img_features = model_feature_extract(img_tensor)\n",
        "    return img_features\n",
        "\n",
        "# for index in range(data_img_groups.shape[0]):\n",
        "\n",
        "#   images_path = data_img_groups.iloc[index][\"images\"]\n",
        "\n",
        "#   images_emmbeded = []\n",
        "\n",
        "#   for path in images_path:\n",
        "#     image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "#     image = preprocess(image).unsqueeze(0)\n",
        "\n",
        "#     img_embedding = get_model_embedding(model_feature_extract, image)\n",
        "#     images_emmbeded.append(img_embedding.flatten())\n",
        "\n",
        "#   images_embbeding = torch.stack(images_emmbeded).mean(dim=0)\n",
        "\n",
        "#   mapeo[index] = images_embbeding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "def get_avg_embbeding(index):\n",
        "    def get_model_embedding(model_feature_extract, img_tensor):\n",
        "        with torch.no_grad():\n",
        "            # Get the image features from the ResNet-152 model\n",
        "            img_features = model_feature_extract(img_tensor)\n",
        "        return img_features\n",
        "\n",
        "    images_path = data_img_groups.iloc[index][\"images\"]\n",
        "\n",
        "    images_emmbeded = []\n",
        "\n",
        "    for path in images_path:\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        image = preprocess(image).unsqueeze(0)\n",
        "\n",
        "        img_embedding = get_model_embedding(model_feature_extract, image)\n",
        "        images_emmbeded.append(img_embedding.flatten())\n",
        "\n",
        "    images_embbeding = torch.stack(images_emmbeded).mean(dim=0)\n",
        "\n",
        "    return images_embbeding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pool_size = multiprocessing.cpu_count()\n",
        "\n",
        "pool_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "def parallarel_procesing(numbers, pool_size):\n",
        "    pool = multiprocessing.Pool(pool_size)\n",
        "    results = pool.map(get_avg_embbeding, numbers)\n",
        "\n",
        "    return results\n",
        "\n",
        "index_images = range(5)#range(data_img_groups.shape[0])\n",
        "\n",
        "results = parallarel_procesing(index_images, pool_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37368"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_img_groups.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json \n",
        "\n",
        "mapeo_ = dict(zip(list(data_img_groups[\"id_annonce\"].values), [ list(x.numpy()) for x in results]))\n",
        "\n",
        "with open(\"average_embbedings_images.json\", \"w\") as outfile: \n",
        "    json.dump(mapeo_, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "M0JZdPD-9J8G"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_features,\n",
        "        data_target,\n",
        "        data_img_groups,\n",
        "        group='train',\n",
        "        transform=None\n",
        "        ) -> None:\n",
        "\n",
        "        self.group = group\n",
        "\n",
        "        self.data_features = data_features.query(\n",
        "            f\"group == '{self.group}'\"\n",
        "            )\n",
        "\n",
        "        self.data_target = data_target.query(\n",
        "            f\"group == '{self.group}'\"\n",
        "            )\n",
        "\n",
        "        self.data_img_groups = data_img_groups.query(\n",
        "            f\"group == '{self.group}'\"\n",
        "            )\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        self.model = models.resnet152(pretrained=True)\n",
        "        self.model_layers = list(self.model.children())[:-1]\n",
        "        self.model_feature_extract = torch.nn.Sequential(*self.model_layers)\n",
        "\n",
        "        self.set_model_eval()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_features)\n",
        "\n",
        "    def set_model_eval(self):\n",
        "      \"\"\"\n",
        "      Set to evaluation model.\n",
        "      \"\"\"\n",
        "      return self.model.eval()\n",
        "\n",
        "\n",
        "    def get_model_embedding(self, img_tensor):\n",
        "\n",
        "      with torch.no_grad():\n",
        "          # Get the image features from the ResNet-152 model\n",
        "          img_features = self.model_feature_extract(img_tensor)\n",
        "\n",
        "      return img_features\n",
        "\n",
        "    def load_image(self, path) -> Image.Image:\n",
        "\n",
        "        return Image.open(path).convert(\"RGB\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        images_path = self.data_img_groups.iloc[index][\"images\"]\n",
        "\n",
        "        images_emmbeded = []\n",
        "\n",
        "        for path in images_path:\n",
        "          image = self.load_image(path)\n",
        "\n",
        "          if self.transform:\n",
        "            image = self.transform(image).unsqueeze(0)\n",
        "\n",
        "            img_embedding = self.get_model_embedding(image)\n",
        "            images_emmbeded.append(img_embedding.flatten())\n",
        "\n",
        "        images_embbeding = torch.stack(images_emmbeded).mean(dim=0)\n",
        "\n",
        "        features = self.data_features.drop(\n",
        "            columns=[\"id_annonce\",\"group\"]\n",
        "            ).iloc[index,:]\n",
        "\n",
        "        #features = torch.from_numpy(features).float()\n",
        "\n",
        "        target = self.data_target[[\"price_scaled\"]].values[index]\n",
        "        #target = np.array(target)\n",
        "        #target = torch.from_numpy(target).float()\n",
        "\n",
        "        #sample = {}\n",
        "\n",
        "        avg_embbeding = images_embbeding\n",
        "        tab_features = torch.tensor(features.values, dtype=torch.float32)\n",
        "        target = torch.tensor(target, dtype=torch.float32)\n",
        "\n",
        "        return avg_embbeding, tab_features, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TDDgkOQOkLV",
        "outputId": "e4005885-dd0c-4462-fbe3-00cf20f92521"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "c:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "c:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\MI30743\\github\\bourbaki_challenges-main\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "train_dataset = CustomDataset(\n",
        "        data_features=X_reorganized.head(100),\n",
        "        data_target=y_reorganized.head(100),\n",
        "        data_img_groups=data_img_groups,\n",
        "        group='train',\n",
        "        transform=preprocess\n",
        "    )\n",
        "\n",
        "val_dataset = CustomDataset(\n",
        "        data_features=X_reorganized.sample(100),\n",
        "        data_target=y_reorganized.sample(100),\n",
        "        data_img_groups=data_img_groups,\n",
        "        group='val',\n",
        "        transform=preprocess\n",
        ")\n",
        "\n",
        "test_dataset = CustomDataset(\n",
        "        data_features=X_reorganized.sample(100),\n",
        "        data_target=y_reorganized.sample(100),\n",
        "        data_img_groups=data_img_groups,\n",
        "        group='test',\n",
        "        transform=preprocess\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "y6itsJqnYbAI"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Asqxg6x4We54"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "scESiD9JY8P3"
      },
      "outputs": [],
      "source": [
        "# Define the neural network\n",
        "class PricePredictionModel(nn.Module):\n",
        "    def __init__(self, embedding_size=2048, tabular_input_size=26):  # Adjust tabular_input_size based on your actual features\n",
        "        super(PricePredictionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_size+tabular_input_size, 512)  # Output a single value for price prediction\n",
        "        self.fc2 = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, image_inputs, tabular_inputs):\n",
        "        combined_embedding = torch.cat((image_inputs, tabular_inputs), dim=1)\n",
        "        output = self.fc1(combined_embedding)\n",
        "        output = self.fc2(output)\n",
        "        return output.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BKotqzgTZixX"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "else:\n",
        "    device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tGCCQowaZi2R",
        "outputId": "934c2d8f-5d3a-4c43-c75a-a7cba3959c89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda:0'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YGtTd4WRZi7O"
      },
      "outputs": [],
      "source": [
        "#model = MultiModal(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "118krSqaZFRQ"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss, and optimizer\n",
        "model = PricePredictionModel().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pD8LtwV5ZjFU"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience, verbose=False, delta=0, path=\"checkpoint.pt\"):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        return self.early_stop\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                f\"Validation Loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model to {self.path}\"\n",
        "            )\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "    def load_checkpoint(self, model):\n",
        "        \"\"\"Loads the best model weights from the saved checkpoint.\"\"\"\n",
        "        model.load_state_dict(torch.load(self.path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W32QV5DNBwsL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mpb3tDPdZjKH"
      },
      "outputs": [],
      "source": [
        "# Tunners\n",
        "LR = 0.01\n",
        "EPOCHS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_i0YzX7eZjP6"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=LR, amsgrad=True, weight_decay=LR * 0.1)\n",
        "\n",
        "criterion = nn.L1Loss().to(device) #nn.L1Loss().to(device)\n",
        "\n",
        "early_stopping = EarlyStopping(patience=int(EPOCHS * 0.5), verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xJzzvICtZjUf"
      },
      "outputs": [],
      "source": [
        "def train_eval(model, loader, optimizer, loss_func, is_training):\n",
        "    loss_list = []\n",
        "    stop = False\n",
        "    model.train() if is_training else model.eval()\n",
        "\n",
        "    with torch.set_grad_enabled(is_training):  # Enable gradients only in training\n",
        "        for batch_idx, (avg_embbeding, tab_features, target) in tqdm(\n",
        "            enumerate(loader),\n",
        "            desc=\"Epoch_Train\" if is_training else \"Epoch_Val\",\n",
        "            total=len(loader),\n",
        "        ):\n",
        "            avg_embbeding, tab_features, target = avg_embbeding.to(device), tab_features.to(device), target.to(device)\n",
        "\n",
        "            if is_training:\n",
        "                optimizer.zero_grad()  # Reset gradients\n",
        "            output = model(avg_embbeding, tab_features)\n",
        "            loss = loss_func(output, target.squeeze(1))\n",
        "\n",
        "            if is_training:\n",
        "                loss.backward()  # Compute gradients\n",
        "                optimizer.step()  # Update weights\n",
        "\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "    if not is_training:\n",
        "        stop = early_stopping(\n",
        "            (sum(loss_list) / len(loss_list)), model\n",
        "        )  # Early stopping decision\n",
        "\n",
        "    return loss_list, stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "O5lfUWvLZjZh"
      },
      "outputs": [],
      "source": [
        "def calc_acc_loss(total_loss, loss):\n",
        "    total_loss.append(sum(loss) / len(loss))\n",
        "\n",
        "    #total_accuracy.append(100 * correct / len(loader.dataset))\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tFF5y0uDZjgA"
      },
      "outputs": [],
      "source": [
        "# Lists\n",
        "total_loss_train = []\n",
        "total_loss_val = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "EO-nMUADGtDP"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "try:\n",
        "  torch.cuda.reset_peak_memory_stats()\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cc3f4de78aab4d2aa678320a7c79e687",
            "137e83a11d934a1ca1ea22d3c00f48db",
            "ba640f217aed44659caf4f7dcb30226c",
            "e6d15e422afb48b1bef40884efee0781",
            "eb0eb9def29c4c6ca88bd9234ed61ac7",
            "892f0a5940e24d7c9f9d68ecb04decfb",
            "63e7b9c816b64106a883f99b4fdc051a",
            "b03de206aab543628464de2a5a7643f6",
            "8a95298173974a2984a32ef622ade051",
            "f7f569be39294134952660ccf99e8cf9",
            "dee1a6237f6947dd8a42d90974b966bc"
          ]
        },
        "id": "ice8f7K4GtHW",
        "outputId": "b5fca034-dccf-43a5-b8f4-7b0595ead390"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9abc3dc674664e59b51ffc5bd1969636",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch_Train:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cb0b0e666034ce78f5bb478d6ded536",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch_Val:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss decreased (inf --> 3939.195801). Saving model to checkpoint.pt\n",
            "Epoch 1/1 - Train Loss: 710.4358, Val Loss: 3939.1958\n",
            "CPU times: total: 6min 21s\n",
            "Wall time: 1min 20s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Training and Evaluation loop\n",
        "for epoch in range(EPOCHS):\n",
        "    # Train Iteration\n",
        "    loss_train, _ = train_eval(\n",
        "        model, train_loader, optimizer, criterion, is_training=True\n",
        "    )\n",
        "\n",
        "    # Val Iteration\n",
        "    loss_val, stop = train_eval(\n",
        "        model, val_loader, optimizer, criterion, is_training=False\n",
        "    )\n",
        "\n",
        "    # Calculate and record loss & accuracy for training and validation\n",
        "    total_loss_train = calc_acc_loss(\n",
        "        total_loss_train,\n",
        "        loss_train\n",
        "    )\n",
        "\n",
        "    # Calculate Loss and Accuracy\n",
        "    total_loss_val = calc_acc_loss(\n",
        "        total_loss_val,\n",
        "        loss_val\n",
        "    )\n",
        "\n",
        "    # Print metrics per epoch\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {total_loss_train[-1]:.4f}, Val Loss: {total_loss_val[-1]:.4f}\"\n",
        "    )\n",
        "\n",
        "    if stop:\n",
        "        print(\"Early Stopping\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siEm14ENrNaw"
      },
      "source": [
        "## Texting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "LkW1txhMrLms"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.reset_peak_memory_stats()\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Kk8f_QSGrLqm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PricePredictionModel(\n",
              "  (fc1): Linear(in_features=2074, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_pred = PricePredictionModel().to(device) #MultiModal(device=device)\n",
        "model_pred.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "model_pred.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "WMZRMt2UZjnJ"
      },
      "outputs": [],
      "source": [
        "def test(model, loader):\n",
        "    model.eval()\n",
        "    total_loss_test = []\n",
        "    outputs_test = []\n",
        "    target_test = []\n",
        "\n",
        "    pred = torch.Tensor().to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (avg_embbeding, tab_features, target) in tqdm(\n",
        "            enumerate(loader), desc=\"Test Epoch\", total=len(loader)\n",
        "        ):\n",
        "            avg_embbeding, tab_features, target = avg_embbeding.to(device), tab_features.to(device), target.to(device)\n",
        "            output = model(avg_embbeding, tab_features)\n",
        "\n",
        "            outputs_test.append(output)\n",
        "            target_test.append(target)\n",
        "\n",
        "            loss = criterion(output, target.squeeze(1))\n",
        "            total_loss_test.append(loss.item())\n",
        "\n",
        "        outputs_test = torch.concat(outputs_test, dim=0).cpu().numpy()\n",
        "        target_test = torch.concat(target_test, dim=0).cpu().numpy()\n",
        "\n",
        "        return total_loss_test, outputs_test, target_test.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "PrV_Ty7gZjr-"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22887364e92f486389abd846c491fe2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Test Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "total_loss_test, outputs_test, target_test = test(model_pred, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "LUUP4n-dZj4J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss Test: 929.151\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loss Test: {(sum(total_loss_test) / len(total_loss_test)):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "T3_HgriYZkDo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "929.1509"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "mean_absolute_error(outputs_test, target_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "137e83a11d934a1ca1ea22d3c00f48db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_892f0a5940e24d7c9f9d68ecb04decfb",
            "placeholder": "​",
            "style": "IPY_MODEL_63e7b9c816b64106a883f99b4fdc051a",
            "value": "Epoch_Train:   0%"
          }
        },
        "63e7b9c816b64106a883f99b4fdc051a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "892f0a5940e24d7c9f9d68ecb04decfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a95298173974a2984a32ef622ade051": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b03de206aab543628464de2a5a7643f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba640f217aed44659caf4f7dcb30226c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b03de206aab543628464de2a5a7643f6",
            "max": 24,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a95298173974a2984a32ef622ade051",
            "value": 0
          }
        },
        "cc3f4de78aab4d2aa678320a7c79e687": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_137e83a11d934a1ca1ea22d3c00f48db",
              "IPY_MODEL_ba640f217aed44659caf4f7dcb30226c",
              "IPY_MODEL_e6d15e422afb48b1bef40884efee0781"
            ],
            "layout": "IPY_MODEL_eb0eb9def29c4c6ca88bd9234ed61ac7"
          }
        },
        "dee1a6237f6947dd8a42d90974b966bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6d15e422afb48b1bef40884efee0781": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7f569be39294134952660ccf99e8cf9",
            "placeholder": "​",
            "style": "IPY_MODEL_dee1a6237f6947dd8a42d90974b966bc",
            "value": " 0/24 [00:00&lt;?, ?it/s]"
          }
        },
        "eb0eb9def29c4c6ca88bd9234ed61ac7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7f569be39294134952660ccf99e8cf9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
